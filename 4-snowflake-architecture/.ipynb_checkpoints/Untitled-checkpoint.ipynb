{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01521041-8172-4f7b-8990-fdf01f62909f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b163c48-2a71-4323-b0d9-0f0ebfbc82e2",
   "metadata": {},
   "source": [
    "Snowflake isnt strictly a \"shared everything\" db. From the docs:\n",
    " Snowflake’s architecture is a hybrid of traditional shared-disk and shared-nothing database architectures. Similar to shared-disk architectures, Snowflake uses a central data repository for persisted data that is accessible from all compute nodes in the platform. But similar to shared-nothing architectures, Snowflake processes queries using MPP (massively parallel processing) compute clusters where each node in the cluster stores a portion of the entire data set locally. This approach offers the data management simplicity of a shared-disk architecture, but with the performance and scale-out benefits of a shared-nothing architecture.\n",
    "https://docs.snowflake.com/en/user-guide/intro-key-concepts.html\n",
    "\n",
    "Jeff Katz (Jigsaw labs )  1 hour ago\n",
    "Yea - but I still don't understand how it's caching strategy is an upgrade over traditional shared everything.  Traditionally shared everything also caches locally, and then have to deal with the underlying data changing (edited) \n",
    "\n",
    "Teej:snowflake:  1 hour ago\n",
    "It's an upgrade because it's very typical for a data warehouse to be 5 min / 1 hour / 1 day behind realtime. Snowflake and many other data warehouses assume you will not have a ton of write contention. So you dont the underlying data isnt really changing that much\n",
    "\n",
    "Teej:snowflake:  1 hour ago\n",
    "If anything, most of our really big tables are append only (event data) which works brilliantly in the model, because 99% of the blocks wont be replaced, just new blocks added\n",
    "\n",
    "James Weakley:omnata:  18 minutes ago\n",
    "then have to deal with the underlying data changing\n",
    "It's more about the efficient way this is done, leveraging the cheapness of storage as Teej described, to ensure that a high performance metadata layer can effortlessly keep up with changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95ab56-4c5f-4f43-916d-84ea1f83d749",
   "metadata": {},
   "source": [
    "https://www.snowflake.com/blog/how-foundationdb-powers-snowflake-metadata-forward/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b215842-497b-446f-8535-566aabb7ea70",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=KkeyjFMmIf8&ab_channel=TheLinuxFoundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fb422-a9eb-411f-9dcf-c54405d54185",
   "metadata": {},
   "source": [
    "https://hevodata.com/blog/snowflake-architecture-cloud-data-warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e871bd-1a2b-47bb-8134-45e58ee7de74",
   "metadata": {},
   "source": [
    "[Snowflake Micropartitions](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd96f08-aa2b-4538-86b8-c085074db621",
   "metadata": {},
   "source": [
    "### More Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40f389-10da-4f3f-beb9-9e8eaf802196",
   "metadata": {},
   "source": [
    "> Mike Taveirne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b25e0a-f554-4ba4-8976-4ab77d9fbad8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "my understanding is snowflake caches data from recent queries up to some amount within cluster nodes on fast disk.  dunno about memory.\n",
    "\n",
    "> The only answer I've seen is that snowflake has a query optimizer that checks to see if the underlying data has changed.\n",
    "\n",
    "\n",
    "i speculate part of that optimization is saying whether the local layer has been invalidated or can be reused.\n",
    "\n",
    "> A client then tries to update record 1 so that ‘foo’ becomes ‘bar’. To do this in a consistent manner the DBMS must take a distributed lock on all nodes that may have cached record 1. Such distributed locks become slower and slower as you increase the number of machines in the cluster and as a result can impede the scalability of the writing process.\n",
    "\n",
    "note none of the databases this 2009 article had an architecture like snowflake in which the compute could be 100% separated from the writing either - a vertica or netezza massively parallel processing database still had the same compute hitting the same finite disks for writes and reads.  whereas a reports_warehouse on snowflake and etl_warehouse on snowflake are fully independent compute, independently reading/writing blobs to what can scale up and down and probably presents more like infinite disk.\n",
    ">  In other words, if snowflake is caching data in the compute layer, but the underlying data changes in S3, doesn't snowflake run into the same caching issues of other shared everything architectures? I get that in an analytics database, there are fewer updates, but then why wouldn't traditional shared-everything architectures work?\n",
    "\n",
    "\n",
    "even zero local caching on snowflake will represent massive improvements over a traditional shared everything database.  the data warehouse problem is typically massive reads and filtration, vs. the operational database problem of single record locates/writes/retrievals.  snowflakes massive reads are based on the brute force amount of clusters you bring to bear on a query, but also being able to read from infinite 'disk' as the source data is not located on a finite amount of disk with finite throughput, but object storage retrieval from S3 which will scale up as more is demanded from it.  then there's additional cleverness around reading only the data where your location lives (the micro-partitions of interest) performing your restrict operation, and those micro-partitions themselves being of a columnar format aiding in the select/project operation.  now you've accomplished the heavy lifting of i/o, you have a much smaller nugget of data left to actually join/aggregate/transform, etc. on.\n",
    "a shared everything/memory, like single node oracle, is going to be tied to a local amount of finite disk and be set up for the row based operations its been designed around.  so it's not going to catch a snowflake on that i/o read, what is often taking the most effort.\n",
    "a shared disk (oracle rac for a row system, netezza for a row system with more cleverness and dedicated hardware/fpgas, a vertica, etc.) all offer greater compute with parallel nodes, and different approaches at speeding up the i/o.  i used to work with/am a huge fan of netezza, many of my netezza buddies ended up at snowflake, and it's the only one of this bunch also designed to be philosophically simple as well like snowflake from the start.  iirc netezza cached absolutely nothing for the longest time, and in its latest fpga iteration, i think the very little if anything still.  it would still crush any single node shared everything system of any type handily for analytic query execution.\n",
    "note also snowfake is caching query results - forgot how long they last - but these can be any size due to s3 as well.  a shared everything database typically won't be caching any, and/or have a small cache, and/or have a very expensive cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbb022-5ff3-4852-8b95-bd21384572aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
